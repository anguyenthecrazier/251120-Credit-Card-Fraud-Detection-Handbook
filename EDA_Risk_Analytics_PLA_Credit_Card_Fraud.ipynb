{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anguyenthecrazier/251120-Credit-Card-Fraud-Detection-Handbook/blob/main/EDA_Risk_Analytics_PLA_Credit_Card_Fraud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG9DAKX_EE7f",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
        "\n",
        "It contains only numerical input variables which are the result of a Principal Component Analysis (PCA) transformation.\n",
        "\n",
        "Due to confidentiality issues, there are not provided the original features and more background information about the data.\n",
        "- Features V1, V2, ... V28 are confidential features, the principal components obtained with PCA;\n",
        "The only features which have not been transformed with PCA are Time and Amount. Feature\n",
        "- Time contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature Amount is the transaction\n",
        "- Amount, this feature can be used for example-dependant cost-senstive learning.\n",
        "Feature Class is the response variable and it takes value 1 in case of fraud and 0 otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DZxXPNKtCbP",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# **1. Library importing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi9ei8Twm6Pe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "style.use('ggplot')\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "%matplotlib inline\n",
        "pd.options.mode.chained_assignment = None\n",
        "import matplotlib.font_manager\n",
        "from scipy.stats import norm\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, auc\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from typing import Dict, List\n",
        "from sklearn import ensemble\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import plotly.figure_factory as ff\n",
        "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMc5ua_u_hIi",
        "outputId": "31db05c7-506a-4d1a-aaf3-fab83b708b61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (0.14.5)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (2.0.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.16.3)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.0.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade statsmodels\n",
        "import statsmodels.api as sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEzDdF5D_hIk",
        "outputId": "6749c14c-039e-425d-86a5-6b0aec0ea3db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtBA93Ru_hIk"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy_0i5bv_hIl",
        "outputId": "80dc620c-bfff-4a80-c765-6f9fe87649bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Hi\\\\Downloads\\\\creditcard.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1490042209.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"C:\\\\Users\\\\Hi\\\\Downloads\\\\creditcard.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Hi\\\\Downloads\\\\creditcard.csv'"
          ]
        }
      ],
      "source": [
        "#Data load\n",
        "file_path = \"C:\\\\Users\\\\Hi\\\\Downloads\\\\creditcard.csv\"\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaF5cPBCzCT9",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# **2. Data exploration analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m--E0pVC2-2v"
      },
      "outputs": [],
      "source": [
        "data = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acBdv3riwkNF"
      },
      "outputs": [],
      "source": [
        "# Number of observations\n",
        "num_observations = data.shape[0]\n",
        "print(\"Number of observations in the dataframe:\", num_observations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHv4CIwUygNS"
      },
      "outputs": [],
      "source": [
        "# Dimension exploration\n",
        "print(\"Number of rows:\", data.shape[0])\n",
        "print(\"Number of columns:\", data.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaEhSj2-uaof"
      },
      "outputs": [],
      "source": [
        "#Structural summary\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK2WUfBFxolF"
      },
      "source": [
        "So most of variables in dataframe is float, while class is integer. Also, there are no missing value in this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwyWovsdxhr0"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DUsAJpGaUWu"
      },
      "source": [
        "The data set contains 284,807 transactions. The mean value of all transactions is 88.35USD while the largest transaction recorded in this data set amounts to 25,691USD. However, as you might be guessing right now based on the mean and maximum, the distribution of the monetary value of all transactions is heavily right-skewed. The vast majority of transactions are relatively small and only a tiny fraction of transactions comes even close to the maximum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4KfU8WYZru1"
      },
      "source": [
        "## **2.1. Class**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zENrITNsv1G6"
      },
      "source": [
        "'Class' is an important feature in this dataset as it labels whether the transaction is normal transaction (Class 0) or fraud transaction (Class 1). In the EDA part, we first understand variable 'Class', then explore its relationships with other variables, including Amount, Time and other anonymous variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVcrkNsd0D--"
      },
      "outputs": [],
      "source": [
        "# Number of transactions under each class\n",
        "data['Class'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmbiAdSCYXck"
      },
      "outputs": [],
      "source": [
        "# Adjustment for poster format\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# Set the graph\n",
        "plt.rcParams['font.family'] = 'arial'\n",
        "\n",
        "ax = sns.countplot(x='Class', data=data, palette='pastel', width=0.5, hue='Class', legend=False)\n",
        "plt.title('Distribution of Legitimate and Fraudulent Transaction', fontsize=20, weight='bold')\n",
        "plt.xlabel('Transaction Class', fontsize=16, labelpad=20)\n",
        "plt.ylabel('Count', fontsize=16, labelpad=20)\n",
        "plt.xticks([0, 1], ['Legitimate (Class 0)', 'Fraudulent (Class 1)'], fontsize=14)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "# Add count labels on top of each bar\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.annotate(f'{int(height)}',\n",
        "                (p.get_x() + p.get_width() / 2., height),\n",
        "                ha='center', va='bottom', fontsize=12, weight='bold')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A comparison of the transaction classes shows that there are 284,315 legitimate transactions (Class 0) and only 492 fraudulent transactions (Class 1). In percentage terms, fraudulent transactions make up only about 0.172% of the entire dataset. This analysis faces the limitations including a strong class imbalance, anonymized data limits interpretability, and the fact that the dataset reflects transactions from a single region and period. Hence, model generalization may be limited."
      ],
      "metadata": {
        "id": "WEDMZeRg_nfx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kROrjD5dZ-iZ",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## **2.2. Amount by Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ea4C4FAwuJp"
      },
      "outputs": [],
      "source": [
        "#Comparing Transaction Amounts\n",
        "\n",
        "data.groupby('Class')['Amount'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The summary shows that the mean of fraudulent transaction amount is higher than that of legitimate one. However, the median of fraudulent transaction amount is approximately 9.25 indicating large discrepancy between the mean and median. This means that most fraudulent transactions involve small amounts, but a few extremely large transactions are pulling the mean upwards.  "
      ],
      "metadata": {
        "id": "W3p3vufqB0Hj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eMf7afRwxAf"
      },
      "outputs": [],
      "source": [
        "# Adjustment for poster format\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# Set a default sans-serif font to avoid warnings\n",
        "plt.rcParams['font.family'] = 'arial'\n",
        "plt.rcParams['font.sans-serif'] = ['arial']\n",
        "\n",
        "\n",
        "# Setting boxplot by using seaborn\n",
        "sns.boxplot(x='Class', y='Amount', data=data, palette='pastel', width=0.3, hue='Class', legend=False, # Changed palette to 'pastel'\n",
        "            boxprops=dict(edgecolor='none'),  # Remove box outline\n",
        "            whiskerprops=dict(color=\"darkgrey\"),\n",
        "            capprops=dict(color='darkgrey'),\n",
        "            medianprops=dict(color='lightgrey'),\n",
        "            flierprops=dict(markeredgecolor='darkgrey', markerfacecolor='darkgrey', markersize=5)\n",
        "           )\n",
        "\n",
        "# changing scale of y-axis to logarithm\n",
        "plt.yscale('log')\n",
        "\n",
        "# Adding title and lable\n",
        "plt.title('Distribution of Transaction Amount by Class', fontsize=20, weight='bold')\n",
        "plt.xlabel('Transaction Class', fontsize=16, labelpad=50)\n",
        "plt.ylabel('Transaction Amount (Logarithmic Scale)', fontsize=16, labelpad=50) # Increased labelpad to increase distance\n",
        "\n",
        "# Setting lable of x-axis\n",
        "plt.xticks([0, 1], ['Legitimate (Class 0)', 'Fraudulent (Class 1)'], fontsize=14)\n",
        "plt.yticks(fontsize=12) # Adjust y-tick font size\n",
        "\n",
        "# Adding grid for better readability\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7, color='lightgrey') # Changed grid line color\n",
        "\n",
        "# Removing spines for a cleaner look\n",
        "sns.despine(left=True, bottom=True)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the wide range of the ‘Amount’ feature, both the boxplot and histogram use a logarithmic scale for clearer visualization. The boxplot reveals the stability of legitimate transactions. The interquartile range (IQR, the middle 50% of data) of legitimate transactions is concentrated in a relatively narrow value range. This suggests consistent and predictable spending pattern for normal transactions. In contrast, the IQR of fraudulent transactions is very widespread, ranging from around 1 to over 100. This indicates that fraudulent behavior is inconsistent and more random in terms of transaction value"
      ],
      "metadata": {
        "id": "fcfzol7iBXp3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACKwI-EIw8UJ"
      },
      "outputs": [],
      "source": [
        "# Create a distribution plot for 'Amount' by 'Class'\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.histplot(data=data, x='Amount', hue='Class', palette='pastel', bins=50, kde=True, stat='density', common_norm=False, edgecolor='none')\n",
        "\n",
        "#Adding Title and Lable\n",
        "plt.title('Distribution of Transaction Amount by Class', fontsize=20, weight='bold')\n",
        "plt.xlabel('Amount', fontsize=16, labelpad=20)\n",
        "plt.ylabel('Density', fontsize=16, labelpad=20)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5F70UfYxCF5",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#Log Transformation\n",
        "data['LogAmount'] = np.log1p(data['Amount'])  # log(Amount + 1) to handle zeros\n",
        "sns.histplot(data=data, x='LogAmount', hue='Class', palette='pastel', bins=50, kde=True, stat='density', common_norm=False, edgecolor='none')\n",
        "\n",
        "#Adding Title and Lable\n",
        "plt.title('Log-Transformed Amount by Class', fontsize=20, weight='bold')\n",
        "plt.xlabel('Log-Amount', fontsize=16, labelpad=20)\n",
        "plt.ylabel('Density', fontsize=16, labelpad=20)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j9a1qGRcBkBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fraudulent transactions have an irregular distribution, with a high frequency of small-amount transactions and several significant spikes at high-amount transactions. This pattern likely indicates a fraudulent strategy to test card validation by making small-amount transactions, followed by a few high-amount transaction in terms of maximizing profit before the card is blocked."
      ],
      "metadata": {
        "id": "fZIJpYJRBdII"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU6Z739LajFh"
      },
      "source": [
        "## **2.3. Time by Class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EegEgpi1amTg"
      },
      "outputs": [],
      "source": [
        "#Comparing Transactions Time\n",
        "\n",
        "data.groupby('Class')['Time'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An analysis of the ‘Time’ feature finds the mean time for fraudulent is lower than that of legitimate one. This is the first indication that fraudulent transactions, on average, tend to occur earlier in this two-day observation period. This trend is confirmed by the median values where half of all fraudulent transactions occurred before second 75,568.50, whereas for legitimate transactions occurred later at second 84."
      ],
      "metadata": {
        "id": "l1RW1q70Bx9b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySraniQAx0CB"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "#Histogram\n",
        "sns.histplot(data=data, x='Time', hue='Class', bins=50, kde=True, stat='density', common_norm=False, palette='pastel', edgecolor='none')\n",
        "\n",
        "#Adding Title and Lable\n",
        "plt.title('Distribution of Transaction Time by Class', fontsize=20, weight='bold')\n",
        "plt.xlabel('Time (Sec)', fontsize=16, labelpad=20)\n",
        "plt.ylabel('Density', fontsize=16, labelpad=20)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Legitimate transactions appear to have a stable pattern, reflecting a normal activity where high volume of transactions is made during daylight hours. In contrast to fraudulent transactions, the distribution pattern presents random and has an inverse pattern. To properly analyse this daily cycle, it is necessary to convert the ‘Time’ feature to represent the hour of the day to capture the cyclical nature of human behaviour"
      ],
      "metadata": {
        "id": "uE0Th2xSB9zY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G63I18hlx5CQ"
      },
      "outputs": [],
      "source": [
        "#Daily Transaction Comparison\n",
        "\n",
        "# Calculate seconds in 24 hours (86400 seconds = 24 hours)\n",
        "seconds_in_day = data['Time'] % 86400\n",
        "data['Hour_of_Day'] = (seconds_in_day // 3600).astype(int)\n",
        "\n",
        "#Data Visualization\n",
        "#Divide graph for 0 and 1\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12), sharex=True)\n",
        "#Plot for legitimate transactions (0)\n",
        "sns.countplot(x='Hour_of_Day', data=data[data['Class'] == 0], ax=ax1, color='#a1c9f4')\n",
        "ax1.set_title('The Legitimate Transaction Distribution in a day', fontsize=14)\n",
        "ax1.set_ylabel('Count', fontsize=14, labelpad=20)\n",
        "ax1.set_xlabel('')\n",
        "#Plot for Fraudulent transactions (1)\n",
        "sns.countplot(x='Hour_of_Day', data=data[data['Class'] == 1], ax=ax2, color='#FFCC99', legend=0)\n",
        "ax2.set_title('The Fraudulent Transaction Distribution in a day', fontsize=14)\n",
        "ax2.set_xlabel('Hours a day (00.00 - 23.00)', fontsize=14, labelpad=20)\n",
        "ax2.set_ylabel('Count', fontsize=14, labelpad=20)\n",
        "#Adding Title and lable\n",
        "plt.suptitle('The Comparison of Daily Transaction', fontsize=18, y=0.92, weight='bold')\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.92])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1qEQ1OfFEsg"
      },
      "source": [
        "Fraudulent transactions have a distribution more even than valid transactions - are equaly distributed in time, including the low real transaction times, during night in Europe timezone. The data shows a clear peak in fraudulent transactions around 02.00 AM, when human activity is minimal. However, relying solely on this pattern would be misleading. Fraudulent transactions also occur during peak hours, as seen at 11.00 AM and in the afternoon. This suggests that analysis by hour of the day is unreliable feature for detecting fraud, as transactions happen during both peak and off-peak hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcBNbYwu_hIq"
      },
      "source": [
        "## **2.3. PCA Variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xFfT7e-GGtf"
      },
      "outputs": [],
      "source": [
        "style.use('ggplot')\n",
        "sns.set_style('whitegrid')\n",
        "plt.subplots(figsize = (30,30))\n",
        "## Plotting heatmap. Generate a mask for the upper triangle (taken from seaborn example gallery)\n",
        "mask = np.zeros_like(data.corr(), dtype=np.bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "sns.heatmap(data.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0,);\n",
        "plt.title(\"Heatmap of all the Features of Train data set\", fontsize = 25);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwlsGVjKGgxt"
      },
      "source": [
        "As can see, some of our predictors do seem to be correlated with the Class variable. Nonetheless, there seem to be relatively little significant correlations for such a big number of variables. This can probably be attributed to two factors:\n",
        "\n",
        "- The data was prepared using a PCA, therefore our predictors are principal components.\n",
        "- The huge class imbalance might distort the importance of certain correlations with regards to our class variable.\n",
        "\n",
        "Because the correlation between variables and 'Class' are so small, the project choosed significant variables are the ones that have correlations with ‘Class’ ≥ 0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tLHf77M_hIq"
      },
      "outputs": [],
      "source": [
        "corr = data.corr()\n",
        "plt.figure(figsize=(30,30))\n",
        "sns.heatmap((corr[(corr>=0.1) |(corr <=-0.1)]), mask=np.triu(corr),annot=True,center = 0,cmap=\"icefire\")\n",
        "plt.suptitle('Heatmap correlation')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMnK2n8_tpmt"
      },
      "source": [
        "We can see that V17 and V14 are the two variables that most related to the Class with the absolute correlations ≥ 0.3. Other important features are V1, V3, V4, V7, V10, V11, V12, V14, V16. V17, V18."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These findings suggest that simple linear models or static rule-based systems will likely be ineffective. Hence, the use of advanced, non-linear machine learning algorithms is essential."
      ],
      "metadata": {
        "id": "VEK_otGsCc06"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s84yb7Dv_hIq"
      },
      "outputs": [],
      "source": [
        "nrows, ncols = len(data.columns[1:29]), 2\n",
        "fig, ax = plt.subplots(nrows, ncols, figsize = (15, 40))\n",
        "\n",
        "for idx, col in enumerate(data.columns[1:29]):\n",
        "\n",
        "    plt.subplot(nrows, ncols, ncols*idx+1)\n",
        "    sns.histplot(data = data, x = col, bins = 30, kde = True)\n",
        "\n",
        "    plt.subplot(nrows, ncols, ncols*idx+2)\n",
        "    sns.boxplot(data = data, x = col, orient = True)\n",
        "\n",
        "plt.suptitle('Distributions of other variables')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lpja6dcq_hIr"
      },
      "outputs": [],
      "source": [
        "# Target'Class' column: 0 = non-fraud, 1 = fraud\n",
        "nrows, ncols = len(data.columns[1:29]), 2\n",
        "fig, ax = plt.subplots(nrows, ncols, figsize=(15, 40))\n",
        "\n",
        "for idx, col in enumerate(data.columns[1:29]):\n",
        "    # Histogram with separate lines for Fraud and Non-Fraud\n",
        "    plt.subplot(nrows, ncols, ncols*idx + 1)\n",
        "    sns.histplot(data[data['Class'] == 0][col], bins=30, kde=True, color='skyblue', label='Non-Fraud', alpha=0.6)\n",
        "    sns.histplot(data[data['Class'] == 1][col], bins=30, kde=True, color='tomato', label='Fraud', alpha=0.6)\n",
        "    plt.legend()\n",
        "    plt.title(f'Distribution of {col}')\n",
        "\n",
        "    # Boxplot (optional: can also split by class)\n",
        "    plt.subplot(nrows, ncols, ncols*idx + 2)\n",
        "    sns.boxplot(data=data, x='Class', y=col, palette=['skyblue', 'tomato'])\n",
        "    plt.title(f'Boxplot of {col} by Class')\n",
        "    plt.xticks([0, 1], ['Non-Fraud', 'Fraud'])\n",
        "\n",
        "plt.suptitle('Distributions and Boxplots of Features by Class', fontsize=16, fontweight='bold')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDW89_Vk_hIr"
      },
      "source": [
        "**Comment:**\n",
        "\n",
        "1. Most feature histograms appear sharp or irregular rather than bell-shaped. This indicates that applying a StandardScaler may not be appropriate, as the features are not normally distributed.\n",
        "\n",
        "2. Many features show a high concentration of values near 0, suggesting strong dimensionality reduction effects—likely due to the PCA transformation, as mentioned in the data introduction.\n",
        "\n",
        "3. Values associated with fraudulent transactions are sparse because the dataset is highly imbalanced. However, these values tend to appear at different ranges or in the tails of the feature distributions. Therefore, RobustScaler may not be suitable for this dataset, as it relies on the median and IQR, effectively ignoring the tails where anomalies occur. This could make the model less sensitive to rare events.\n",
        "\n",
        "4. The boxplots confirm that fraud cases frequently appear outside the central quartile range, particularly in highly correlated features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kys6vWlj1p8I"
      },
      "source": [
        "## **2.4. EDA Conclusion**\n",
        "\n",
        "1.\tThe finding on ‘Amount’ feature indicates that most fraudulent transactions involve small amounts, but a few extremely large transactions. This pattern likely indicates a fraudulent strategy to test card validation by making small-amount transactions, followed by a few high-amount transaction in terms of maximazing profit before the card is blocked.\n",
        "\n",
        "2.\tThe finding on ‘Time’ feature shows which fraudulent transactions often occur when human activity decreases, such as in the late night (02.00 AM). However, fraudulent transactions also occur during peak hours, as seen at 11.00 AM and in the afternoon. This suggests that analysis by hour of the day is unreliable feature for detecting fraud, as transactions happen during both peak and off-peak hours.\n",
        "\n",
        "3.\tThe correlation analysis reveals that no features stand out as highly correlated (> 0.5 or ≤ 0.5) with fraud. This indicates that fraud is not a simple linear function but instead determined by complex, multivariate patterns. This finding suggests that simple linear models or static rule-based systems will likely be ineffective. Hence, the use of advanced, non-linear machine learning algorithms is essential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH-pXOi5ZRaP"
      },
      "source": [
        "# **3. Unsupervised Learning**\n",
        "\n",
        "**Popular Unsupervised Algorithms for Anomaly Detection**\n",
        "- Isolation Forest: This algorithm isolates each point and finds the outliers (fraud). It's fast and works well on large datasets.\n",
        "https://machinelearninggeek.com/outlier-detection-using-isolation-forests/\n",
        "\n",
        "- One-Class SVM: This method looks at where normal transactions happen and flags anything unusual as potential fraud.\n",
        "- Autoencoders: These are neural networks that try to recreate normal transactions. If they struggle to recreate something, it might be fraud.\n",
        "- Local Outlier Factor (LOF): This algorithm checks the density of transactions around each point. If a transaction has fewer similar neighbors, it might be fraud.\n",
        "- DBSCAN: This algorithm looks for groups of similar transactions (clusters). If a transaction doesn’t belong to any group, it might be an outlier or fraud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El4D_RBjaxdI",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## **3.1. Feature transformation**\n",
        "\n",
        "Feature transformation plays an essential role in anomaly detection. Many anomaly detection techniques work best when the data follows a Gaussian (normal) distribution. If our features are skewed (i.e., they have long tails in one direction) or are on different scales, these algorithms may struggle to accurately identify normal data points and anomalies. This could lead to normal instances being wrongly flagged as outliers or genuine anomalies being overlooked.\n",
        "\n",
        "According to our histogram plots, many of our features are highly skewed. To address this issue, we will apply log transformations; therefore, we will stabilize variance, reduce skewness, and enhance the interpretability of the data. This ensures that the characteristics of the data align more closely with the assumptions of the model, thereby improving its ability to accurately identify anomalies and enhancing the overall reliability of the detection process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gp6WQ03izky"
      },
      "outputs": [],
      "source": [
        "#Features name\n",
        "features = df.columns[:-1]\n",
        "\n",
        "# Create a copy of the DataFrame to avoid changing the original\n",
        "df_transformed = df.copy()\n",
        "\n",
        "# Function to handle log transformation for skewed data\n",
        "def log_transform_skewed(column):\n",
        "    # For positive and zero values (log1p avoids log(0) errors)\n",
        "    transformed = np.where(column >= 0, np.log1p(column), -np.log1p(-column))\n",
        "    return transformed\n",
        "\n",
        "# Compute skewness before transformation\n",
        "skewness_before = df.skew()\n",
        "\n",
        "# Apply transformation to skewed columns\n",
        "for col in features:\n",
        "    if abs(df[col].skew()) > 0.75:  # Threshold for skewness\n",
        "        df_transformed[col] = log_transform_skewed(df[col])\n",
        "\n",
        "# Compute skewness after transformation\n",
        "skewness_after = df_transformed.skew()\n",
        "\n",
        "# Compare skewness before and after\n",
        "skewness_comparison = pd.DataFrame({\n",
        "    'Skewness Before': skewness_before,\n",
        "    'Skewness After': skewness_after\n",
        "})\n",
        "\n",
        "# Print the comparison\n",
        "skewness_comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmmIq_oSjRKk"
      },
      "outputs": [],
      "source": [
        "# Set up the figure; 10 rows (10*3=30 subplots), adjust as needed\n",
        "fig, axes = plt.subplots(10, 3, figsize=(15, 40))  # Adjust rows to fit all features\n",
        "\n",
        "# Flatten axes array to loop through easily\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot each feature in a separate subplot\n",
        "for i, feature in enumerate(features):\n",
        "    sns.histplot(df_transformed[feature], ax=axes[i], kde=False, bins=30)\n",
        "    axes[i].set_title(f'{feature} after Transformation')\n",
        "    axes[i].set_xlabel(feature)\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "\n",
        "# Remove any unused subplots if features < 30\n",
        "for i in range(len(features), len(axes)):\n",
        "    fig.delaxes(axes[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3JX2kyKjVdC"
      },
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X = df_transformed[features]\n",
        "y = df_transformed['Class']\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Standardize the data\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EGhTO42_hIy"
      },
      "outputs": [],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq82GngNahj_",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## **3.2. Isolation Forest**\n",
        "This technique works by randomly selecting features and splitting data points. Anomalies, or outliers, are easier to isolate, resulting in shorter paths in the \"forest.\" It's effective for large datasets, as it can quickly identify anomalies without needing to model the data's distribution. However, it may miss complex patterns since it relies on simple random splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_urBB8n_hIy"
      },
      "outputs": [],
      "source": [
        "# Initialize the Isolation Forest\n",
        "iso_forest = IsolationForest(contamination=0.05, random_state=101)   # high contamination to catch more fraud transactions\n",
        "\n",
        "# Fit the model and predict (returns -1 for anomalies and 1 for normal data)\n",
        "iso_preds = iso_forest.fit_predict(X_scaled)\n",
        "\n",
        "# Convert -1 (anomalies) to 1 (fraud) and 1 (normal) to 0 (non-fraud)\n",
        "iso_preds = [1 if x == -1 else 0 for x in iso_preds]\n",
        "\n",
        "# Evaluate the results\n",
        "print(classification_report(y, iso_preds, digits=4)) #CHỖ NÀY\n",
        "roc_auc_iso = roc_auc_score(y, iso_preds)\n",
        "print(f\"ROC AUC Score:, {roc_auc_iso:.5f}\") #CHỖ NÀY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0bEiUlRkkj7"
      },
      "outputs": [],
      "source": [
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(y, iso_preds)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, cmap = \"icefire\", fmt='g')\n",
        "\n",
        "# Add labels, title, and axis ticks\n",
        "plt.title('Confusion Matrix ')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.xticks([0.5, 1.5], ['Non-Fraud (0)', 'Fraud (1)'])\n",
        "plt.yticks([0.5, 1.5], ['Non-Fraud (0)', 'Fraud (1)'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUfEM7sRlX6z"
      },
      "source": [
        "The **Isolation Forest model** achieves **high overall accuracy** (95%) and a strong ROC AUC (0.90), indicating it’s generally good at distinguishing fraudulent from normal transactions.\n",
        "However, its precision for the fraud class is very low (0.03), meaning it raises many false alarms.\n",
        "On the other hand, recall is high (0.85), showing it successfully identifies most frauds.\n",
        "This makes the model suitable for initial anomaly detection where catching all frauds is critical, but it would need post-filtering or secondary validation to reduce false positives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XezhDfuS_hIz"
      },
      "outputs": [],
      "source": [
        "# Get anomaly scores (the lower, the more anomalous)\n",
        "iso_scores = -iso_forest.decision_function(X_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjjjPsLw_hIz"
      },
      "outputs": [],
      "source": [
        "# Compute ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y, iso_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=1.5, linestyle='--', label='Random guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve – Isolation Forest Fraud Detection', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Also print ROC AUC using the raw scores\n",
        "roc_auc_score_value = roc_auc_score(y, iso_scores)\n",
        "print(\"ROC AUC (using anomaly scores):\", roc_auc_score_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCvLpMFt_hIz"
      },
      "source": [
        "**Comment**\n",
        "Here, the project also calculated ROC-AUC score, using a continuous anomaly score. This new ROC AUC evaluates the model's ability to rank fraud higher than normal across all posible thresholds. On the other hand, the previous ROC AUC score that indicated in the classification report is the Binary ROC AUC which sees only 0 for normal and 1 for fraud transaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd781UmojyfG",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## **3.3. One Class SVM**\n",
        "This model learns the boundary of normal data points and flags any points outside this boundary as anomalies. It's particularly powerful in high-dimensional spaces where traditional methods may struggle. However, it can be computationally intensive and sensitive to the presence of noise in the data, which can lead to misclassification.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdhE8T1zpw1b"
      },
      "outputs": [],
      "source": [
        "# Initialize One-Class SVM\n",
        "oc_svm = OneClassSVM(kernel='rbf', gamma=0.001, nu=0.05)\n",
        "\n",
        "# Fit the model and predict (returns -1 for anomalies and 1 for normal data)\n",
        "svm_preds = oc_svm.fit_predict(X_scaled)\n",
        "\n",
        "# Convert -1 (anomalies) to 1 (fraud) and 1 (normal) to 0 (non-fraud)\n",
        "svm_preds = [1 if x == -1 else 0 for x in svm_preds]\n",
        "\n",
        "# Evaluate the results\n",
        "print(classification_report(y, svm_preds, digits=4))\n",
        "roc_auc_svm = roc_auc_score(y, svm_preds)\n",
        "print(f\"ROC AUC Score:, {roc_auc_svm:.5f}\") #CHỖ NÀY\n",
        "print(\"Confusion Matrix:\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0agIpuhvmbNr"
      },
      "outputs": [],
      "source": [
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(y, svm_preds)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, cmap = \"icefire\", fmt='g')\n",
        "\n",
        "# Add labels, title, and axis ticks\n",
        "plt.title('Confusion Matrix ')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.xticks([0.5, 1.5], ['Non-Fraud (0)', 'Fraud (1)'])\n",
        "plt.yticks([0.5, 1.5], ['Non-Fraud (0)', 'Fraud (1)'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Djvw34M_hIz"
      },
      "outputs": [],
      "source": [
        "# Get continuous anomaly scores from the SVM\n",
        "# Higher scores → more normal; lower → more anomalous\n",
        "svm_scores = -oc_svm.decision_function(X_scaled)  # negate to align higher = more likely fraud\n",
        "\n",
        "# Compute ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y, svm_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=1.5, linestyle='--', label='Random guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve – One-Class SVM Fraud Detection', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Optional: print ROC AUC using continuous scores\n",
        "roc_auc_score_value = roc_auc_score(y, svm_scores)\n",
        "print(\"ROC AUC (using continuous scores):\", roc_auc_score_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-class SVM shows similar patterns recall with Isolation Forest (0.8496) but having a slightly worse performance; furthermore, its time-consuming nature may limit its practical application in real-time fraud detection systems"
      ],
      "metadata": {
        "id": "4kfMknTmHKu4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY96LajfmeKb",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## **3.4. Local Outlier Factor (LOF)**\n",
        " LOF assesses the local density of each data point compared to its neighbors. If a point has a significantly lower density than those around it, it's considered an outlier. This technique is useful for identifying anomalies in varying densities but can be sensitive to the choice of the number of neighbors, which affects its performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnOPHStomoTl"
      },
      "outputs": [],
      "source": [
        "# Initialize Local Outlier Factor (LOF)\n",
        "lof = LocalOutlierFactor(n_neighbors=10, contamination=0.05)\n",
        "\n",
        "# Predict (returns -1 for anomalies and 1 for normal data)\n",
        "lof_preds = lof.fit_predict(X_scaled)\n",
        "\n",
        "# Convert -1 (anomalies) to 1 (fraud) and 1 (normal) to 0 (non-fraud)\n",
        "lof_preds = [1 if x == -1 else 0 for x in lof_preds]\n",
        "\n",
        "# Evaluate the results\n",
        "print(classification_report(y, lof_preds, digits=4))\n",
        "roc_auc_lof = roc_auc_score(y, lof_preds)\n",
        "print(f\"ROC AUC Score:, {roc_auc_lof:.5f}\") #CHỖ NÀY\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y, lof_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7Mh1xxKmyFM"
      },
      "outputs": [],
      "source": [
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(y, lof_preds)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, cmap = \"icefire\", fmt='g')\n",
        "\n",
        "# Add labels, title, and axis ticks\n",
        "plt.title('Confusion Matrix ')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.xticks([0.5, 1.5], ['Non-Fraud (0)', 'Fraud (1)'])\n",
        "plt.yticks([0.5, 1.5], ['Non-Fraud (0)', 'Fraud (1)'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCNPLb8B_hI0"
      },
      "outputs": [],
      "source": [
        "# Get anomaly scores (higher = more anomalous)\n",
        "lof_scores = -lof.negative_outlier_factor_\n",
        "\n",
        "# Compute ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y, lof_scores)\n",
        "roc_auc = roc_auc_score(y, lof_scores)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=1.5, linestyle='--', label='Random guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve - Local Outlier Factor', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Optional: print ROC AUC using continuous scores\n",
        "roc_auc_score_value = roc_auc_score(y, lof_scores)\n",
        "print(\"ROC AUC (using continuous scores):\", roc_auc_score_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13V7ndAd_hI0"
      },
      "source": [
        "The result for LOF even worse than random guess, once again intepreted that this is the worse model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOtV_MvpyLny",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## **3.5. Autoencoders**\n",
        "\n",
        "These are neural networks designed to compress and reconstruct data. By training on normal transactions, they learn to recreate them effectively. If a transaction cannot be reconstructed well, it's flagged as an anomaly. Autoencoders are great for capturing complex patterns in data, but they require more computational resources and careful tuning of architecture and parameters to perform effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwP4t5ax_hI0"
      },
      "outputs": [],
      "source": [
        "# Define the autoencoder model\n",
        "def build_autoencoder(input_dim):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "\n",
        "      # Encoder\n",
        "    encoded = Dense(32, activation='relu', kernel_regularizer=l2(0.001))(input_layer)\n",
        "    encoded = Dropout(0.2)(encoded)\n",
        "    encoded = Dense(16, activation='relu', kernel_regularizer=l2(0.001))(encoded)\n",
        "    encoded = Dense(8, activation='relu', kernel_regularizer=l2(0.001))(encoded)\n",
        "\n",
        "  # Latent space\n",
        "    latent = Dense(4, activation='relu')(encoded)\n",
        "\n",
        "# Decoder\n",
        "    decoded = Dense(8, activation='relu', kernel_regularizer=l2(0.001))(latent)\n",
        "    decoded = Dropout(0.2)(decoded)\n",
        "    decoded = Dense(16, activation='relu', kernel_regularizer=l2(0.001))(decoded)\n",
        "    decoded = Dense(32, activation='relu', kernel_regularizer=l2(0.001))(decoded)\n",
        "    output_layer = Dense(input_dim, activation='linear')(decoded)\n",
        "\n",
        "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
        "    return autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC6XLVXSyRlS"
      },
      "outputs": [],
      "source": [
        "# Build and compile the model\n",
        "autoencoder = build_autoencoder(X_scaled.shape[1])\n",
        "autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mse')\n",
        "\n",
        "# Train the model on normal transactions (non-fraudulent class, y == 0)\n",
        "X_train = X_scaled[y == 0]\n",
        "autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, shuffle=True, validation_split=0.1)\n",
        "\n",
        "#Train on only the normal transaction data (y==0) to learn \"normal behaviors, normal features\" of the non-fraud transaction\n",
        "\n",
        "# Calculate reconstruction error for all transactions\n",
        "reconstructed = autoencoder.predict(X_scaled)\n",
        "ae_mse = np.mean(np.power(X_scaled - reconstructed, 2), axis=1)\n",
        "\n",
        "# Set a threshold for anomaly detection\n",
        "threshold = np.percentile(ae_mse, 90)  # Adjust threshold (90th percentile)\n",
        "autoen_preds = np.where(ae_mse > threshold, 1, 0)  # 1: anomaly (fraud), 0: normal\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y, autoen_preds, digits=4))\n",
        "roc_auc_ae = roc_auc_score(y, autoen_preds)\n",
        "print(f\"ROC AUC Score:, {roc_auc_ae:.5f}\") #CHỖ NÀY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxREYd-Oy54C"
      },
      "outputs": [],
      "source": [
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(y, autoen_preds)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, cmap=\"icefire\", fmt='g')\n",
        "\n",
        "# Add labels, title, and axis ticks\n",
        "plt.title('Confusion Matrix ')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.xticks([0.5, 1.5], ['Non-Fraud (0)', 'Fraud (1)'])\n",
        "plt.yticks([0.5, 1.5], ['Non-Fraud (0)', 'Fraud (1)'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX4v4JcicGRD"
      },
      "outputs": [],
      "source": [
        "# Use reconstruction errors (MSE) as continuous anomaly scores\n",
        "fpr, tpr, thresholds = roc_curve(y, ae_mse)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=1.5, linestyle='--', label='Random guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve – Autoencoder Fraud Detection', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoencoders showed a strong balance between recall (0.8943) and accuracy (0.90). Despite its low precision (0.0154), its ability to capture most frauds makes it suitable and worthwhile for trade off, since it is often better to flag more potential cases for review than to miss real frauds."
      ],
      "metadata": {
        "id": "mV6a5OIGIiLR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-Be_dsgFco7"
      },
      "source": [
        "# **4. Unsupervised Learning Methods with High-correlated features**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project further analysed the application of these 2 effective unsupervised learning algorithms by testing only on features with absolute correlations with ‘Class’ ≥ 0.1. The result shows that removing weak variables can reduce noise and allow the models to focus on the most relevant patterns and improve the accuracy of Isolation Forest and Autoencoder"
      ],
      "metadata": {
        "id": "o0ljtnx1IvDX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi7J0I-8Ydh3",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## **4.1. Data preparation (High-correlated set)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maXiCKMYBeqV"
      },
      "outputs": [],
      "source": [
        "# Features that want to keep\n",
        "cols_to_keep = ['V1', 'V3', 'V4', 'V7', 'V10', 'V11', 'V12', 'V14', 'V16', 'V17', 'V18']\n",
        "\n",
        "# Z with high-correlated features\n",
        "Z = X[cols_to_keep]\n",
        "\n",
        "# Print Z\n",
        "print(Z.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYPGgE96_hI1"
      },
      "outputs": [],
      "source": [
        "# Standardize the data\n",
        "Z_scaled = scaler.fit_transform(Z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "zue1qbvW_hI1"
      },
      "source": [
        "## **4.2. Isolation Forest with High-correlated variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zizy2Q-2_hI1"
      },
      "outputs": [],
      "source": [
        "# Initialize the Isolation Forest\n",
        "iso_forest_imp = IsolationForest(contamination=0.05, random_state=101)   # high contamination to catch more fraud transactions\n",
        "\n",
        "# Fit the model and predict (returns -1 for anomalies and 1 for normal data)\n",
        "iso_imp_preds = iso_forest_imp.fit_predict(Z_scaled)\n",
        "\n",
        "# Convert -1 (anomalies) to 1 (fraud) and 1 (normal) to 0 (non-fraud)\n",
        "iso_imp_preds = [1 if x == -1 else 0 for x in iso_imp_preds]\n",
        "\n",
        "# Evaluate the results\n",
        "print(classification_report(y, iso_imp_preds, digits=4))\n",
        "roc_imp_auc = roc_auc_score(y, iso_imp_preds)\n",
        "print(f\"ROC AUC Score: {roc_imp_auc:.5f}\") #CHỖ NÀY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VYt3E2c_hI2"
      },
      "outputs": [],
      "source": [
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(y, iso_imp_preds)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, cmap = \"icefire\", fmt='g')\n",
        "\n",
        "# Add labels, title, and axis ticks\n",
        "plt.title('Confusion Matrix ')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.xticks([0.5, 1.5], ['Non-Fraud (0)', 'Fraud (1)'])\n",
        "plt.yticks([0.5, 1.5], ['Non-Fraud (0)', 'Fraud (1)'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drdchRUE_hI2"
      },
      "outputs": [],
      "source": [
        "# Get anomaly scores (the lower, the more anomalous)\n",
        "iso_imp_scores = -iso_forest_imp.decision_function(Z_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0M1nhV3_hI2"
      },
      "outputs": [],
      "source": [
        "# Compute ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y, iso_imp_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=1.5, linestyle='--', label='Random guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve – Isolation Forest Fraud Detection - Important Features', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Also print ROC AUC using the raw scores\n",
        "roc_auc_score_value = roc_auc_score(y, iso_imp_scores)\n",
        "print(\"ROC AUC (using anomaly scores):\", roc_auc_score_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method achieved higher recall for fraudulent transactions, increasing from 0.8516 to 0.8963. The F1-scores for the fraud class also improved slightly for Isolation Forest, reflecting a better balance between identifying frauds and limiting the number of false positives"
      ],
      "metadata": {
        "id": "4HRaEGo7Iyob"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "pAWdnlDs_hI2"
      },
      "source": [
        "## **4.3. Autoencoder with High-correlatd variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "abruua1n_hI2"
      },
      "outputs": [],
      "source": [
        "# Build and compile the model\n",
        "autoencoder_imp = build_autoencoder(Z_scaled.shape[1])\n",
        "autoencoder_imp.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mse')\n",
        "\n",
        "# Train the model on normal transactions (non-fraudulent class, y == 0)\n",
        "Z_train = Z_scaled[y == 0]\n",
        "autoencoder_imp.fit(Z_train, Z_train, epochs=50, batch_size=32, shuffle=True, validation_split=0.1)\n",
        "\n",
        "#Train on only the normal transaction data (y==0) to learn \"normal behaviors, normal features\" of the non-fraud transaction\n",
        "\n",
        "# Calculate reconstruction error for all transactions\n",
        "reconstructed_imp = autoencoder_imp.predict(Z_scaled)\n",
        "ae_mse_imp = np.mean(np.power(Z_scaled - reconstructed_imp, 2), axis=1)\n",
        "\n",
        "# Set a threshold for anomaly detection\n",
        "threshold = np.percentile(ae_mse_imp, 90)  # Adjust threshold (90th percentile)\n",
        "autoen_imp_preds = np.where(ae_mse_imp > threshold, 1, 0)  # 1: anomaly (fraud), 0: normal\n",
        "\n",
        "# Evaluate the model\n",
        "print(classification_report(y, autoen_imp_preds, digits=4))  #CHỖ NÀY\n",
        "roc_auc_ae_imp = roc_auc_score(y, autoen_imp_preds)\n",
        "print(f\"ROC AUC Score: {roc_auc_ae_imp:.5f}\") #CHỖ NÀY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J59zkZR_hI2"
      },
      "outputs": [],
      "source": [
        "# Plot the confusion matrix\n",
        "cm = confusion_matrix(y, autoen_imp_preds)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, cmap=\"icefire\", fmt='g')\n",
        "\n",
        "# Add labels, title, and axis ticks\n",
        "plt.title('Confusion Matrix ')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.xticks([0.5, 1.5], ['Non-Fraud (0)', 'Fraud (1)'])\n",
        "plt.yticks([0.5, 1.5], ['Non-Fraud (0)', 'Fraud (1)'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwLmzyVW_hI2"
      },
      "outputs": [],
      "source": [
        "# Use reconstruction errors (MSE) as continuous anomaly scores\n",
        "fpr, tpr, thresholds = roc_curve(y, ae_mse_imp)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=1.5, linestyle='--', label='Random guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve – Autoencoder Fraud Detection', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method achieved higher recall for fraudulent transactions, increasing from 0.8516 to 0.8963 for the Autoencoder. Both predictives were able to detect a larger proportion of actual fraud cases after weak features were excluded. These results suggest that using unsupervised learning on higher correlation features further enhances the models’ sensitivity to meaningful anomalies, leading to stronger and more efficient unsupervised fraud detection performance."
      ],
      "metadata": {
        "id": "4gKh-tj5JcEe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD7jbzgV_hI2"
      },
      "source": [
        "# **5. Supervised Learning Methods**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw9gP9Dv_hI3"
      },
      "source": [
        "**Machine Learning**\n",
        "In this part, different supervised learning methods are applied to detect fraudulent transactions\n",
        "- RandomForestClassifier()\n",
        "- XGBClassifier()\n",
        "- LGBMClassifier()\n",
        "- CatBoostClassifier()\n",
        "- AdaBoostClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZABl1m8O_hI3"
      },
      "source": [
        "## **5.1. Data preposessing with SMOTE**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the performance of different machine learning algorithms, the dataset is processed with SMOTE, and then separated into 2 small datasets, “train” and “test”, and then it is tested again with the full original dataframe, which has not gone through any kind of processing. This will ensure the most accurate assessment and help us to recognise the possibility of real-life application."
      ],
      "metadata": {
        "id": "oBsdQl4RKBgG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vZYns7R_hI3"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.over_sampling import SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7lBibQX_hI3"
      },
      "outputs": [],
      "source": [
        "# Define dataset\n",
        "def Definedata(df):\n",
        "    X_smote = df.drop(columns=['Class']).values\n",
        "    y_smote = df['Class'].values\n",
        "    return X_smote, y_smote"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiKjox47_hI3"
      },
      "outputs": [],
      "source": [
        "def ApplySMOTE(df, test_size=0.5, random_state=2):\n",
        "    X_smote, y_smote = Definedata(df)\n",
        "\n",
        "    # summarize class distribution\n",
        "    counter = Counter(y_smote)\n",
        "    print(\"Original class distribution:\", counter)\n",
        "\n",
        "    # apply SMOTE\n",
        "    smt = SMOTE(random_state=0)\n",
        "    X_res, y_res = smt.fit_resample(X_smote, y_smote)\n",
        "\n",
        "    # summarize new class distribution\n",
        "    counter_res = Counter(y_res)\n",
        "    print(\"After SMOTE class distribution:\", counter_res)\n",
        "\n",
        "    # split train/test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_res, y_res, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    # optional: scatter plot for first 2 features\n",
        "    for label in np.unique(y_res):\n",
        "        row_ix = np.where(y_res == label)\n",
        "        plt.scatter(X_res[row_ix, 0], X_res[row_ix, 1], label=str(label))\n",
        "    plt.legend()\n",
        "    plt.title(\"SMOTE Data Distribution (first 2 features)\")\n",
        "    plt.show()\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ykx0GKH__hI3"
      },
      "outputs": [],
      "source": [
        "# Train model and plot confusion matrices\n",
        "def Models(model, X_train, X_test, y_train, y_test, df, title=\"Model\", cmap='icefire'):\n",
        "    # train model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # full dataset\n",
        "    X_all, y_all = Definedata(df)\n",
        "\n",
        "    # confusion matrices\n",
        "    train_matrix = pd.crosstab(y_train, model.predict(X_train), rownames=['Actual'], colnames=['Predicted'])\n",
        "    test_matrix = pd.crosstab(y_test, model.predict(X_test), rownames=['Actual'], colnames=['Predicted'])\n",
        "    full_matrix = pd.crosstab(y_all, model.predict(X_all), rownames=['Actual'], colnames=['Predicted'])\n",
        "\n",
        "    # plot heatmaps\n",
        "    f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize=(20, 4))\n",
        "\n",
        "    sns.heatmap(train_matrix, annot=True, fmt=\"d\", cbar=False, ax=ax1, cmap=cmap)\n",
        "    ax1.set_title(f\"{title} - Training Set\")\n",
        "    ax1.set_xlabel(f\"Accuracy: {accuracy_score(y_train, model.predict(X_train)):.4f}\")\n",
        "\n",
        "    sns.heatmap(test_matrix, annot=True, fmt=\"d\", cbar=False, ax=ax2, cmap=cmap)\n",
        "    ax2.set_title(f\"{title} - Testing Set\")\n",
        "    ax2.set_xlabel(f\"Accuracy: {accuracy_score(y_test, model.predict(X_test)):.4f}\")\n",
        "\n",
        "    sns.heatmap(full_matrix, annot=True, fmt=\"d\", cbar=False, ax=ax3, cmap=cmap)\n",
        "    ax3.set_title(f\"{title} - Full Dataset\")\n",
        "    ax3.set_xlabel(f\"Accuracy: {accuracy_score(y_all, model.predict(X_all)):.4f}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return model, model.predict(X_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFWnNmmX_hI3"
      },
      "outputs": [],
      "source": [
        "# Feature importances\n",
        "def FeatureImportances(model, X_train, y_train, df):\n",
        "    # Fit model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Feature importance\n",
        "    importances = model.feature_importances_\n",
        "    features = df.drop(columns=['Class']).columns\n",
        "\n",
        "    # Create dataframe\n",
        "    imp = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
        "\n",
        "    # Cumulative Importance\n",
        "    imp['Cumulative Importance'] = imp['Importance'].cumsum()\n",
        "\n",
        "    # Sort importance\n",
        "    imp = imp.sort_values(by='Importance', ascending = False).reset_index(drop=True)\n",
        "\n",
        "    return imp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2A4du8C1_hI3"
      },
      "outputs": [],
      "source": [
        "# Apply SMOTE\n",
        "X_train, X_test, y_train, y_test = ApplySMOTE(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za1qXk66_hI3"
      },
      "source": [
        "## **5.2. Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b87biXM_hI3"
      },
      "outputs": [],
      "source": [
        "# Train RandomForest model\n",
        "rf_model = RandomForestClassifier(random_state=0)\n",
        "model, y_pred = Models(rf_model, X_train, X_test, y_train, y_test, df, title=\"RandomForest / SMOTE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWZheJhk_hI4"
      },
      "outputs": [],
      "source": [
        "# Feature importances\n",
        "imp_df = FeatureImportances(rf_model, X_train, y_train, df)\n",
        "print(imp_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZktaSQ3_hI4"
      },
      "outputs": [],
      "source": [
        "# Full dataset\n",
        "X_all, y_all = Definedata(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GTkZjuf_hI4"
      },
      "outputs": [],
      "source": [
        "# Classification report\n",
        "report = classification_report(y_all, y_pred, digits=4)\n",
        "print(\"Classification Report (Full Dataset)\")\n",
        "print(report)\n",
        "\n",
        "# Get probabilities for the positive class (Class=1)\n",
        "y_proba_rf = rf_model.predict_proba(X_all)[:, 1]\n",
        "\n",
        "# ROC-AUC\n",
        "roc_auc_rf = roc_auc_score(y_all, y_proba_rf)\n",
        "print(f\"ROC AUC Score (Full Dataset): {roc_auc_rf:.5f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_all, y_pred)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual_{i}\" for i in range(cm.shape[0])],\n",
        "                     columns=[f\"Pred_{i}\" for i in range(cm.shape[1])])\n",
        "print(\"Confusion Matrix (Full Dataset):\")\n",
        "print(cm_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0P3yh4Z_hI4"
      },
      "outputs": [],
      "source": [
        "# Calculate ROC curve points\n",
        "fpr, tpr, thresholds = roc_curve(y_all, y_proba_rf)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, color='darkorange', label=f'ROC curve (AUC = {roc_auc_rf:.5f})')\n",
        "plt.plot([0,1], [0,1], color='navy', linestyle='--', label='Random guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve - Random Forest', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtgMP8bl_hI4"
      },
      "outputs": [],
      "source": [
        "# Calculate Gini coefficient\n",
        "gini = 2 * roc_auc_rf - 1\n",
        "print(f\"Gini Coefficient: {gini:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest so far is the best performing model with the highest F1-score (0.9685), and according to its confusion matrix, there are no false negatives and 32 false positives"
      ],
      "metadata": {
        "id": "cpfzyQfPKQye"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "wfD6wz4R_hI4"
      },
      "source": [
        "## **5.3. XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS63Hm7Q_hI4"
      },
      "outputs": [],
      "source": [
        "!pip install xgboost\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPHc13fm_hI4"
      },
      "outputs": [],
      "source": [
        "# Train XGBoots model\n",
        "xgb_model = XGBClassifier(random_state=0)\n",
        "model, y_pred = Models(xgb_model, X_train, X_test, y_train, y_test, df, title=\"XGBoot / SMOTE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHDhOtmi_hI4"
      },
      "outputs": [],
      "source": [
        "# Feature importances\n",
        "imp_df = FeatureImportances(xgb_model, X_train, y_train, df)\n",
        "print(imp_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_AUxxIQ_hI4"
      },
      "outputs": [],
      "source": [
        "# Full dataset\n",
        "X_all, y_all = Definedata(df)\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(y_all, y_pred, digits=4)\n",
        "print(\"Classification Report (Full Dataset)\")\n",
        "print(report)\n",
        "\n",
        "# Get probabilities for the positive class (Class=1)\n",
        "y_proba_xgb = xgb_model.predict_proba(X_all)[:, 1]\n",
        "\n",
        "# ROC-AUC\n",
        "roc_auc_xgb = roc_auc_score(y_all, y_proba_xgb)\n",
        "print(f\"ROC AUC Score (Full Dataset): {roc_auc_xgb:.5f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_all, y_pred)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual_{i}\" for i in range(cm.shape[0])],\n",
        "                     columns=[f\"Pred_{i}\" for i in range(cm.shape[1])])\n",
        "print(\"Confusion Matrix (Full Dataset):\")\n",
        "print(cm_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnPmAdaR_hI4"
      },
      "outputs": [],
      "source": [
        "# Calculate ROC curve points\n",
        "fpr, tpr, thresholds = roc_curve(y_all, y_proba_xgb)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, color='darkorange', label=f'ROC curve (AUC = {roc_auc_xgb:.5f})')\n",
        "plt.plot([0,1], [0,1], color='navy', linestyle='--', label='Random guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve - XGBoosts', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ugr-Vsis_hI5"
      },
      "outputs": [],
      "source": [
        "# Calculate Gini coefficient\n",
        "gini_xgb = 2 * roc_auc_xgb - 1\n",
        "print(f\"Gini Coefficient of XGBoosts: {gini_xgb:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "UkVotJW9_hI5"
      },
      "source": [
        "## **5.4. AdaBoost Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG0WJpmL_hI5"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Train AdaBoost model\n",
        "ada_model = AdaBoostClassifier(\n",
        "    random_state=0,\n",
        "    algorithm='SAMME',\n",
        "    learning_rate=0.5,\n",
        "    n_estimators=100\n",
        ")\n",
        "\n",
        "# Fit model and make predictions\n",
        "model, y_pred = Models(ada_model, X_train, X_test, y_train, y_test, df, title=\"AdaBoost / SMOTE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLj7Nn_e_hI5"
      },
      "outputs": [],
      "source": [
        "# Feature importances\n",
        "imp_df = FeatureImportances(ada_model, X_train, y_train, df)\n",
        "print(imp_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRziT4GG_hI5"
      },
      "outputs": [],
      "source": [
        "# Full dataset\n",
        "X_all, y_all = Definedata(df)\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(y_all, y_pred, digits=4)\n",
        "print(\"Classification Report (Full Dataset)\")\n",
        "print(report)\n",
        "\n",
        "# Get probabilities for the positive class (Class=1)\n",
        "y_proba_ada = ada_model.predict_proba(X_all)[:,1]\n",
        "\n",
        "# ROC-AUC\n",
        "roc_auc_ada = roc_auc_score(y_all, y_proba_ada)\n",
        "print(f\"ROC AUC Score (Full Dataset): {roc_auc_ada:.5f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_all, y_pred)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual_{i}\" for i in range(cm.shape[0])],\n",
        "                     columns=[f\"Pred_{i}\" for i in range(cm.shape[1])])\n",
        "print(\"Confusion Matrix (Full Dataset):\")\n",
        "print(cm_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmzyQqzt_hI5"
      },
      "outputs": [],
      "source": [
        "# Calculate ROC curve points\n",
        "fpr, tpr, thresholds = roc_curve(y_all, y_proba_ada)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, color='darkorange', label=f'ROC curve (AUC = {roc_auc_ada:.5f})')\n",
        "plt.plot([0,1], [0,1], color='navy', linestyle='--', label='Random guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve - AdaBoost', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WZGFr3q_hI5"
      },
      "outputs": [],
      "source": [
        "# Calculate Gini coefficient\n",
        "gini_ada = 2 * roc_auc_ada - 1\n",
        "print(f\"Gini Coefficient of AdaBoost: {gini_ada:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "ufyqYlYO_hI5"
      },
      "source": [
        "## **5.5. CatBoost Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBw_eXMe_hI5"
      },
      "outputs": [],
      "source": [
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FgTe1y0_hI6"
      },
      "outputs": [],
      "source": [
        "# Train CatBoost model\n",
        "cat_model = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    learning_rate=0.02,\n",
        "    depth=12,\n",
        "    eval_metric='AUC',\n",
        "    random_seed=42,\n",
        "    bagging_temperature=0.2,\n",
        "    od_type='Iter',\n",
        "    metric_period=100,\n",
        "    od_wait=100\n",
        ")\n",
        "\n",
        "# Fit model and make predictions (using your custom Models() function)\n",
        "model, y_pred = Models(cat_model, X_train, X_test, y_train, y_test, df, title=\"CatBoost / SMOTE\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiQRtHUD_hI6"
      },
      "outputs": [],
      "source": [
        "# Feature importances\n",
        "imp_df = FeatureImportances(cat_model, X_train, y_train, df)\n",
        "print(imp_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tM5bgKKS_hI6"
      },
      "outputs": [],
      "source": [
        "# Full dataset\n",
        "X_all, y_all = Definedata(df)\n",
        "\n",
        "# Classification report\n",
        "report = classification_report(y_all, y_pred, digits=4)\n",
        "print(\"=== Classification Report on Full Dataset ===\")\n",
        "print(report)\n",
        "\n",
        "# Get probabilities for the positive class (Class=1)\n",
        "y_proba_cat = cat_model.predict_proba(X_all)[:,1]\n",
        "\n",
        "# ROC-AUC\n",
        "roc_auc_cat = roc_auc_score(y_all, y_proba_cat)\n",
        "print(f\"ROC AUC Score (Full Dataset): {roc_auc_cat:.5f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_all, y_pred)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual_{i}\" for i in range(cm.shape[0])],\n",
        "                     columns=[f\"Pred_{i}\" for i in range(cm.shape[1])])\n",
        "print(\"Confusion Matrix (Full Dataset):\")\n",
        "print(cm_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVkAv8g__hI6"
      },
      "outputs": [],
      "source": [
        "# Calculate ROC curve points\n",
        "fpr, tpr, thresholds = roc_curve(y_all, y_proba_cat)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, color='darkorange', label=f'ROC curve (AUC = {roc_auc_cat:.5f})')\n",
        "plt.plot([0,1], [0,1], color='navy', linestyle='--', label='Random guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve - CatBoost', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNYjKkgE_hI6"
      },
      "outputs": [],
      "source": [
        "# Calculate Gini coefficient\n",
        "gini_cat = 2 * roc_auc_cat - 1\n",
        "print(f\"Gini Coefficient of CatBoost: {gini_cat:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "5AIxALqH_hI6"
      },
      "source": [
        "## **5.6. Light GBM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayjilpP7_hI6"
      },
      "outputs": [],
      "source": [
        "!pip install lightgbm\n",
        "from lightgbm import LGBMClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxcZmZS2_hI6"
      },
      "outputs": [],
      "source": [
        "# LightGBM parameters\n",
        "params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 7,              # should be smaller than 2^(max_depth)\n",
        "    'max_depth': 4,               # -1 means no limit\n",
        "    'min_child_samples': 100,     # minimum data in one leaf\n",
        "    'max_bin': 100,               # number of bins for feature values\n",
        "    'subsample': 0.9,             # subsample ratio for training data\n",
        "    'subsample_freq': 1,          # frequency of subsampling\n",
        "    'colsample_bytree': 0.7,      # ratio of columns for each tree\n",
        "    'min_child_weight': 0,        # minimum sum of instance weight in a leaf\n",
        "    'min_split_gain': 0,          # minimum loss reduction required to make a split\n",
        "    'nthread': 8,\n",
        "    'verbose': 0,\n",
        "    'scale_pos_weight': 150       # for highly imbalanced data\n",
        "}\n",
        "\n",
        "# Train LightGBM model\n",
        "lgbm_model = LGBMClassifier(random_state=0, **params)\n",
        "\n",
        "# Fit model and make predictions (using your custom Models() function)\n",
        "model, y_pred = Models(lgbm_model, X_train, X_test, y_train, y_test, df, title=\"LightGBM / SMOTE\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2dWQ7Ym_hI6"
      },
      "outputs": [],
      "source": [
        "# Feature importances\n",
        "imp_df = FeatureImportances(lgbm_model, X_train, y_train, df)\n",
        "print(imp_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LK3P_Ck-_hI7"
      },
      "outputs": [],
      "source": [
        "# Full dataset\n",
        "X_all, y_all = Definedata(df)\n",
        "\n",
        "# classification report\n",
        "report = classification_report(y_all, y_pred, digits=4)\n",
        "print(\"Classification Report (Full Dataset)\")\n",
        "print(report)\n",
        "\n",
        "# Get probabilities for the positive class (Class=1)\n",
        "y_proba_lgbm = lgbm_model.predict_proba(X_all)[:,1]\n",
        "\n",
        "# ROC-AUC\n",
        "roc_auc_lgbm = roc_auc_score(y_all, y_proba_lgbm)\n",
        "print(f\"ROC AUC Score (Full Dataset): {roc_auc:.5f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_all, y_pred)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"Actual_{i}\" for i in range(cm.shape[0])],\n",
        "                     columns=[f\"Pred_{i}\" for i in range(cm.shape[1])])\n",
        "print(\"Confusion Matrix (Full Dataset):\")\n",
        "print(cm_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAJnHI86_hI7"
      },
      "outputs": [],
      "source": [
        "# Calculate ROC curve points\n",
        "fpr, tpr, thresholds = roc_curve(y_all, y_proba_lgbm)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, color='darkorange', label=f'ROC curve (AUC = {roc_auc_lgbm:.5f})')\n",
        "plt.plot([0,1], [0,1], color='navy', linestyle='--', label='Random guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve - LightGBM', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWGKSn5P_hI7"
      },
      "outputs": [],
      "source": [
        "# Calculate Gini coefficient\n",
        "gini_lgbm = 2 * roc_auc_lgbm - 1\n",
        "print(f\"Gini Coefficient of CatBoost: {gini_lgbm:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, the results of supervised learning algorithms are much more precise than unsupervised ones. The Random Forest so far is the best performing model with the highest F1-score (0.9685), and according to its confusion matrix, there are no false negatives and 32 false positives. XGBoots and CatBoots both have good recalls and no false negatives, but XGBoots slightly performs better than CatBoots with higher Precision (1) and F1-score (1). On the other hand, AdaBoost and LightGBM have really poor overall balance with F1-score (1) equal to 0.1598 and 0.0144, respectively. LightGBM is the worst-performing with extremely low Precision (1) (0.0072), which is even lower than the Anomaly detection models’ performances, reflecting severe overprediction of the minority class. Furthermore, the results also showed that the most important features for the models are likely the ones that have better correlations with ‘Class’, but not all listed high correlation features have great contributions to the Machine Learning model. Therefore, the outcomes of detecting fraudulent transactions on highly correlated data frames are significantly worse than using the whole original one, indicating that the whole dataframe must be used as an input for machine learning methods and hybrid methods that are based on supervised algorithms."
      ],
      "metadata": {
        "id": "OWIocTfuKdW3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plu5ZkOA_hI7"
      },
      "source": [
        "# **6. Hybrid model**\n",
        "\n",
        "Fraud detection data is typically highly imbalanced and constantly evolving — new fraud patterns appear that are not represented in past labels.\n",
        "To address this, we combine both supervised and unsupervised methods:\n",
        "\n",
        "**Supervised models** (e.g., Random Forest, XGBoost)\n",
        "- Learn from labeled data to recognize known fraud patterns.\n",
        "- Provide high precision and interpretability when past fraud behavior is well-documented.\n",
        "\n",
        "**Unsupervised models** (e.g., Isolation Forest, Autoencoder)\n",
        "- Do not rely on labels, so they can detect new or unseen fraud behaviors.\n",
        "- Identify anomalies that differ significantly from normal transaction patterns.\n",
        "\n",
        "*By combining both:*\n",
        "\n",
        "- We leverage historical knowledge (supervised learning).\n",
        "\n",
        "- We stay adaptive to emerging and unseen frauds (unsupervised learning).\n",
        "\n",
        "This hybrid design improves coverage, robustness, and early detection of fraudulent activities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pdO448o_hI7"
      },
      "source": [
        "## **6.1. Hybrid model with full dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp7V9WU2_hI7"
      },
      "outputs": [],
      "source": [
        "# Use output from Isolation Forest\n",
        "iso_scores = - iso_forest.decision_function(X_scaled)\n",
        "\n",
        "# Use reconstruction error from Autoencoder\n",
        "reconstructions = autoencoder.predict(X_scaled)\n",
        "ae_mse = np.mean(np.power(X_scaled - reconstructions, 2), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNb_R5U6_hI7"
      },
      "outputs": [],
      "source": [
        "# Combine the anomative detection score into the dataframe\n",
        "df_processed = pd.DataFrame(X_scaled, columns=features)\n",
        "df_processed['iso_score'] = iso_scores\n",
        "df_processed['ae_mse'] = ae_mse\n",
        "\n",
        "# Combine the machine learning result into the dataframe\n",
        "df_processed['xgb_pred'] = xgb_model.predict_proba(X_all)[:, 1]\n",
        "df_processed['rf_pred']  = rf_model.predict_proba(X_all)[:, 1]\n",
        "df_processed['cat_pred'] = cat_model.predict_proba(X_all)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXs9ayXz_hI7"
      },
      "outputs": [],
      "source": [
        "# Create hybrid features set that include the predict results\n",
        "hybrid_features = df_processed[['iso_score', 'ae_mse', 'xgb_pred', 'rf_pred', 'cat_pred']]\n",
        "\n",
        "# Train meta-classifier (Logistic Regression)\n",
        "meta_model = LogisticRegression()\n",
        "meta_model.fit(hybrid_features, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Tr1lvsp_hI8"
      },
      "outputs": [],
      "source": [
        "# Predict probability of class 1 (fraud)\n",
        "hybrid_probs = meta_model.predict_proba(hybrid_features)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMyy5d1M_hI8"
      },
      "outputs": [],
      "source": [
        "# Find precision, recall và thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y, hybrid_probs)\n",
        "\n",
        "# Find F1-score for threshold\n",
        "f1_scores = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1] + 1e-10)\n",
        "\n",
        "# Find best threshold\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_idx]\n",
        "print(f\"Best threshold based on F1 for hybrid model: {best_threshold:.4f}\")\n",
        "\n",
        "# Predict labels by best threshold\n",
        "hybrid_pred_labels = (hybrid_probs >= best_threshold).astype(int)\n",
        "\n",
        "# In classification report\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y, hybrid_pred_labels, digits=4))\n",
        "\n",
        "# ROC-AUC\n",
        "roc_auc = roc_auc_score(y, hybrid_probs)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.5f}\")\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(recall, precision, color='green', label='PR Curve')\n",
        "plt.scatter(recall[best_idx], precision[best_idx], color='red',\n",
        "            label=f'Best Threshold = {best_threshold:.4f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Hybrid Model Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjs8amJL_hI8"
      },
      "outputs": [],
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y, hybrid_pred_labels)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, cmap=\"icefire\", fmt='g')\n",
        "\n",
        "# Add labels, title, and axis ticks\n",
        "plt.title('Hybrid Model Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.xticks([0.5, 1.5], ['Non-Fraud (0)', 'Fraud (1)'])\n",
        "plt.yticks([0.5, 1.5], ['Non-Fraud (0)', 'Fraud (1)'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9WZz1vL_hI8"
      },
      "outputs": [],
      "source": [
        "# Calculate ROC curve points\n",
        "fpr, tpr, thresholds = roc_curve(y_all, hybrid_probs)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, color='darkorange', label=f'ROC curve (AUC = {roc_auc:.5f})')\n",
        "plt.plot([0,1], [0,1], color='navy', linestyle='--', label='Random guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve - Hybrid model', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJXniV91_hI8"
      },
      "source": [
        "## **6.2. Example of predicting a transaction with this model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPfbrue9_hI8"
      },
      "outputs": [],
      "source": [
        "# Pick a transaction from the dataframe\n",
        "transaction = df.iloc[[105]].iloc[:, :-1]\n",
        "print(transaction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqKZRznl_hI8"
      },
      "outputs": [],
      "source": [
        "def predict_transaction(transaction, scaler, iso_forest, autoencoder,\n",
        "                            xgb_model, rf_model, cat_model, meta_model, best_threshold):\n",
        "    # Scale\n",
        "    X_scaled_new = scaler.transform(transaction)\n",
        "\n",
        "    # Anomaly scores\n",
        "    iso_score_new = -iso_forest.decision_function(X_scaled_new)[0]\n",
        "    reconstruction_new = autoencoder.predict(X_scaled_new)\n",
        "    ae_mse_new = np.mean(np.power(X_scaled_new - reconstruction_new, 2))\n",
        "\n",
        "    # Supervised predictions\n",
        "    xgb_pred_new = xgb_model.predict_proba(X_scaled_new)[:, 1][0]\n",
        "    rf_pred_new  = rf_model.predict_proba(X_scaled_new)[:, 1][0]\n",
        "    cat_pred_new = cat_model.predict_proba(X_scaled_new)[:, 1][0]\n",
        "\n",
        "    # Hybrid vector\n",
        "    hybrid_vector_new = np.array([[iso_score_new, ae_mse_new, xgb_pred_new, rf_pred_new, cat_pred_new]])\n",
        "\n",
        "    # Meta model prediction\n",
        "    prob_new = meta_model.predict_proba(hybrid_vector_new)[:, 1][0]\n",
        "    label_new = int(prob_new >= best_threshold)\n",
        "\n",
        "    return {\n",
        "        \"iso_score\": iso_score_new,\n",
        "        \"ae_mse\": ae_mse_new,\n",
        "        \"xgb_pred\": xgb_pred_new,\n",
        "        \"rf_pred\": rf_pred_new,\n",
        "        \"cat_pred\": cat_pred_new,\n",
        "        \"hybrid_prob\": prob_new,\n",
        "        \"predicted_label\": label_new,\n",
        "        \"result\": \"FRAUD\" if label_new==1 else \"NON-FRAUD\"\n",
        "    }\n",
        "\n",
        "# Usage\n",
        "result = predict_transaction(transaction, scaler, iso_forest, autoencoder,\n",
        "                                 xgb_model, rf_model, cat_model, meta_model, best_threshold)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "NhG8_YWL_hI9"
      },
      "source": [
        "## **6.3. Hybrid model inputs from applying Unsupervised methods with only high correlated features**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "res2pgBo_hI9"
      },
      "outputs": [],
      "source": [
        "# Use output from Isolation Forest\n",
        "iso_imp_scores = - iso_forest_imp.decision_function(Z_scaled)\n",
        "\n",
        "# Calculate reconstruction error for all transactions\n",
        "reconstructed_imp = autoencoder_imp.predict(Z_scaled)\n",
        "ae_mse_imp = np.mean(np.power(Z_scaled - reconstructed_imp, 2), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAnB871P_hI9"
      },
      "outputs": [],
      "source": [
        "# Combine the anomative detection score into the dataframe\n",
        "df_imp_processed = pd.DataFrame(X_scaled, columns=features)\n",
        "df_imp_processed['iso_imp_score'] = iso_imp_scores\n",
        "df_imp_processed['ae_mse_imp'] = ae_mse_imp\n",
        "\n",
        "# Combine the machine learning result into the dataframe\n",
        "df_imp_processed['xgb_pred'] = xgb_model.predict_proba(X_all)[:, 1]\n",
        "df_imp_processed['rf_pred']  = rf_model.predict_proba(X_all)[:, 1]\n",
        "df_imp_processed['cat_pred'] = cat_model.predict_proba(X_all)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cETJee1_hI9"
      },
      "outputs": [],
      "source": [
        "df_imp_processed.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXLQ2B08_hI-"
      },
      "outputs": [],
      "source": [
        "# Create hybrid features set that include the predict results\n",
        "hybrid_imp_features = df_imp_processed[['iso_imp_score', 'ae_mse_imp', 'xgb_pred', 'rf_pred','cat_pred']]\n",
        "\n",
        "# Train meta-classifier (Logistic Regression)\n",
        "meta_imp_model = LogisticRegression()\n",
        "meta_imp_model.fit(hybrid_imp_features, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T18xYJZ_hI-"
      },
      "outputs": [],
      "source": [
        "# Predict probability of class 1 (fraud)\n",
        "hybrid_imp_probs = meta_imp_model.predict_proba(hybrid_imp_features)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcS6_0c9_hI-"
      },
      "outputs": [],
      "source": [
        "# Find precision, recall và thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y, hybrid_imp_probs)\n",
        "\n",
        "# Find F1-score for threshold\n",
        "f1_scores = 2 * precision[:-1] * recall[:-1] / (precision[:-1] + recall[:-1] + 1e-10)\n",
        "\n",
        "# Find best threshold\n",
        "best_idx = np.argmax(f1_scores)\n",
        "best_threshold = thresholds[best_idx]\n",
        "print(f\"Best threshold based on F1 for hybrid model: {best_threshold:.4f}\")\n",
        "\n",
        "# Predict labels by best threshold\n",
        "hybrid_imp_pred_labels = (hybrid_imp_probs >= best_threshold).astype(int)\n",
        "\n",
        "# In classification report\n",
        "print(\"Classification Report\")\n",
        "print(classification_report(y, hybrid_imp_pred_labels, digits=4))\n",
        "\n",
        "# ROC-AUC\n",
        "roc_auc_hybrid_imp = roc_auc_score(y, hybrid_imp_probs)\n",
        "print(f\"ROC-AUC Score: {roc_auc_hybrid_imp:.5f}\")\n",
        "\n",
        "# Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(recall, precision, color='green', label='PR Curve')\n",
        "plt.scatter(recall[best_idx], precision[best_idx], color='red',\n",
        "            label=f'Best Threshold = {best_threshold:.4f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Hybrid Model Precision-Recall Curve (High-corelated Features)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgivXPwC_hI-"
      },
      "outputs": [],
      "source": [
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y, hybrid_imp_pred_labels)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, cmap=\"icefire\", fmt='g')\n",
        "\n",
        "# Add labels, title, and axis ticks\n",
        "plt.title('Hybrid Model 2 Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.xticks([0.5, 1.5], ['Non-Fraud (0)', 'Fraud (1)'])\n",
        "plt.yticks([0.5, 1.5], ['Non-Fraud (0)', 'Fraud (1)'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75o707V5_hI-"
      },
      "outputs": [],
      "source": [
        "# Calculate ROC curve points\n",
        "fpr, tpr, thresholds = roc_curve(y_all, hybrid_imp_pred_labels)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, color='darkorange', label=f'ROC curve (AUC = {roc_auc:.5f})')\n",
        "plt.plot([0,1], [0,1], color='navy', linestyle='--', label='Random guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve - Hybrid model (High-correlated Features)', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This hybrid approach enhances adaptability, precision, and robustness, enabling the system to detect both historical and emerging fraud effectively. Since fraud detection performed better on a modified data frame with highly correlated variables, the project also tested Hybrid Model 2 using unsupervised method scores from this data frame. The result shows that this approach can increase accuracy, lower the false positive rate, increase the F1-score (0) to 1.00, and the F1-score (1) to 0.9801 with only 20 false positives. Hybrid Model 1 and 2 have the same performance, just different thresholds (0.6100 and 0.5970)."
      ],
      "metadata": {
        "id": "fmaB6bi4Kyi5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF40QphxK9WC"
      },
      "source": [
        "# **7. Summary and Findings**\n",
        "\n",
        "The current project aimed to have a further understanding with real life fraud transaction data and construct, evaluate a comprehensive fraud detection framework. The analysis was conducted using the Credit Card Fraud Detection dataset, which has been used widely for testing and developing different approaches to financial fraud transactions. In the scope of these projects, our exploration data analysis (EDA) and predictive model building effort have generated new findings to this model, which lead to practical implementation and future research.\n",
        "The EDA identified important behavioral and statistical differences between legitimate and fraudulent transactions. Analysing the number of transactions, the result showed that fraudulent transactions generally involved smaller amounts, but occasionally high-value outliers also appeared, reflecting that fraudsters’s strategy is to test card validity with small purchases before conducting large unauthorized ones. ‘Time’ feature further revealed that fraudulent activity is more likely to happen during off-peak hours, particularly around 2:00 am, when legitimate user activity is minimal. Correlation analysis indicated that only 11 of 30 features (V1 - V28, Amount, and Time) had an absolute correlation with fraud greater than 0.1, and none above 0.5, confirming that fraud cannot be captured through linear relationships and requires non-linear, multivariate learning techniques. The distributions of features show that most of the variables, especially higher correlated features are not normally distributed, and the fraud transactions mostly appear in different ranges or in the tails of the feature distribution. The finding suggested that we should not use Robust Scaler or Standard Scaler to preprocess this data before training the models.\n",
        "The project explored the performances of anomaly detection, and machine learning algorithms for fraud detection. Testing revealed that the best unsupervised methods are Isolation Forest and Autoencoder, which yielded effective recall scores (0.85 and 0.89, respectively) but suffered from low precision and high false positive rates, indicating the need for further investigation into machine learning techniques. Supervised algorithms demonstrated superior performance with Random Forest achieving an F1-score of 0.9685 and perfect recall, alongside just 32 false positives. XGBoost and CatBoost also performed well, showcasing high precision (0.8723 and 0.8585, respectively) without false negatives. However, similar to the previous conclusion related to fraud detection in general, applying different methods can increase the performance of the model. The introduction of a hybrid model, integrating outputs from selected supervised and unsupervised models via a Logistic Regression meta-classifier, yielded the best overall performance metrics: an accuracy of 0.9999 and precision of 0.9690, with only 20 false positives. The findings emphasized the crucial role of machine learning and anomaly detection in effectively identifying fraud in financial transactions, particularly as such transactions are often minimal among a multitude of daily transactions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGgfk9WBx-_M"
      },
      "source": [
        "# **8. Reference**\n",
        "\n",
        "https://www.kaggle.com/code/annastasy/anomaly-detection-credit-card-fraud#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_id6iZ-X_hI-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}